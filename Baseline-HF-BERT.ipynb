{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "X_TRAIN = '../dataset/HF-BERT_x_train.npy'\n",
    "Y_TRAIN = '../dataset/HF-BERT_y_train.npy'\n",
    "X_TEST = '../dataset/HF-BERT_x_test.npy'\n",
    "Y_TEST = '../dataset/HF-BERT_y_test.npy'\n",
    "SAVED_MODEL = \"../Baseline-HF-BERT.h5\"\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 17241.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing train data -----------------------------------------------------------------------\n",
    "# load topic class labels\n",
    "print(\"making train dataset...\")\n",
    "with open('../data/topic/classes.txt','r',encoding='utf-8') as f:\n",
    "    labels = f.read().splitlines()\n",
    "topic_class_hypothesis = dict()\n",
    "for i,label in enumerate(labels):\n",
    "    topic_class_hypothesis[i] = 'this text is about ' + ' or '.join([wordnet.synsets(word)[0].definition() for word in label.split(' & ')])\n",
    "\n",
    "# load train data\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "\n",
    "texts = texts_v0\n",
    "\n",
    "# ## example -------------------------------------\n",
    "import random\n",
    "texts = texts.splitlines()\n",
    "texts = random.sample(texts,1000)\n",
    "texts = \"\\n\".join(texts)\n",
    "# ## ---------------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "x_train, y_train = [],[]\n",
    "first, second = [],[]\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "    label,text = label_text.split('\\t')\n",
    "    rand_base = [0,1,2,3,4,5,6,7,8,9]\n",
    "    rand_base.remove(int(label))\n",
    "    label_rand = np.random.choice(rand_base)\n",
    "    first.append(preprocessing(text))\n",
    "    second.append(topic_class_hypothesis[int(label)])\n",
    "    y_train.append(1)\n",
    "    first.append(preprocessing(text))\n",
    "    second.append(topic_class_hypothesis[int(label_rand)])\n",
    "    y_train.append(0)\n",
    "\n",
    "x_train = tokenizer(first, second, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 139s 130ms/step - loss: 0.4754 - binary_accuracy: 0.7475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b23ea49b40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.classifier = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"classifier\")\n",
    "model.compile(optimizer=keras.optimizers.Adam(3e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "model.fit([x_train[\"input_ids\"],x_train[\"attention_mask\"]], y_train, epochs=EPOCH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 41s 40ms/step\n",
      "[[7.9871696e-01]\n",
      " [3.1703088e-04]\n",
      " [8.0799925e-01]\n",
      " ...\n",
      " [3.1677075e-04]\n",
      " [8.0644631e-01]\n",
      " [8.0094749e-01]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.540     0.701      1000\n",
      "           1      0.685     1.000     0.813      1000\n",
      "\n",
      "    accuracy                          0.770      2000\n",
      "   macro avg      0.842     0.770     0.757      2000\n",
      "weighted avg      0.842     0.770     0.757      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([x_train[\"input_ids\"],x_train[\"attention_mask\"]], batch_size=BATCH_SIZE)\n",
    "print(pred.logits)\n",
    "y_pred = np.where(pred.logits<0.5, 0, 1)\n",
    "\n",
    "rep = metrics.classification_report(y_train,y_pred,digits=3)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 5278.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia class ------------------------------------------------------------------------------------------------------\n",
    "with open('../data/dbpedia_csv/classes.txt','r',encoding='utf-8') as f:\n",
    "    classes = f.read().splitlines()\n",
    "    dbpedia_class = ['this text is about '+text for text in classes]\n",
    "\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "import random\n",
    "reader = random.sample(reader,1000)\n",
    "# #----------------------------\n",
    "\n",
    "x_test, y_test = [],[]\n",
    "first, second = [],[]\n",
    "for cls_num,auth,readtext in tqdm(reader,total=len(reader)):\n",
    "    for db_class in dbpedia_class:\n",
    "        text = readtext.replace(auth, \"\")\n",
    "        first.append(preprocessing(text))\n",
    "        second.append(db_class)\n",
    "    y_test.append(int(cls_num)-1)           \n",
    "\n",
    "x_test = tokenizer(first, second, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 297s 42ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.034     0.085     0.049        59\n",
      "        Edu.      0.147     0.319     0.202        72\n",
      "        Art.      0.333     0.015     0.028        68\n",
      "        Ath.      0.000     0.000     0.000        66\n",
      "        Off.      0.133     0.730     0.225        74\n",
      "        Mea.      0.000     0.000     0.000        63\n",
      "        Bui.      0.562     0.118     0.196        76\n",
      "        Nat.      0.206     0.084     0.120        83\n",
      "        Vil.      0.274     0.565     0.369        85\n",
      "        Ani.      0.036     0.014     0.020        73\n",
      "        Pla.      0.000     0.000     0.000        85\n",
      "        Alb.      1.000     0.100     0.182        60\n",
      "        Fil.      0.000     0.000     0.000        76\n",
      "        Wri.      1.000     0.017     0.033        60\n",
      "\n",
      "    accuracy                          0.155      1000\n",
      "   macro avg      0.266     0.146     0.102      1000\n",
      "weighted avg      0.251     0.155     0.106      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([x_test[\"input_ids\"],x_test[\"attention_mask\"]], batch_size=BATCH_SIZE)\n",
    "split_pred = np.array_split(pred.logits,len(y_test))\n",
    "y_pred = [np.argmax(p) for p in split_pred]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "rep = metrics.classification_report(y_test,y_pred,target_names=target_names,digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e32349538976b425bbf8209bec3a52ef38eb988b8b568d4d0fb100f86dbbec2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
