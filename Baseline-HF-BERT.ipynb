{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "X_TRAIN = '../dataset/HF-BERT_x_train.npy'\n",
    "Y_TRAIN = '../dataset/HF-BERT_y_train.npy'\n",
    "X_TEST = '../dataset/HF-BERT_x_test.npy'\n",
    "Y_TEST = '../dataset/HF-BERT_y_test.npy'\n",
    "SAVED_MODEL = \"../Baseline-HF-BERT.h5\"\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel, logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text,auth):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*?\\)','',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # 著者名の削除\n",
    "    text = text.replace(auth,'')\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing train data -----------------------------------------------------------------------\n",
    "# load topic class labels\n",
    "print(\"making train dataset...\")\n",
    "with open('../data/topic/classes.txt','r',encoding='utf-8') as f:\n",
    "    labels = f.read().splitlines()\n",
    "topic_class_hypothesis = dict()\n",
    "for i,label in enumerate(labels):\n",
    "    topic_class_hypothesis[i] = 'this text is about ' + ' or '.join([wordnet.synsets(word)[0].definition() for word in label.split(' & ')])\n",
    "\n",
    "# load train data\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "\n",
    "# ## example -------------------------------------\n",
    "import random\n",
    "texts = texts.splitlines()\n",
    "texts = random.sample(texts,10000)\n",
    "texts = \"\\n\".join(texts)\n",
    "# ## ---------------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "bert = TFAutoModel.from_pretrained(MODEL)\n",
    "\n",
    "x_train, y_train = [],[]\n",
    "first, second = [],[]\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "    label,text = label_text.split('\\t')\n",
    "    rand_base = [0,1,2,3,4,5,6,7,8,9]\n",
    "    rand_base.remove(int(label))\n",
    "    label_rand = np.random.choice(rand_base)\n",
    "    text = preprocessing(text,'')\n",
    "    tokenized = tokenizer(text,topic_class_hypothesis[int(label)],return_tensors=\"tf\",truncation=True, padding=\"max_length\")\n",
    "    bert_feature = bert(tokenized,output_hidden_states=True)\n",
    "    x_train.append(bert_feature.pooler_output[0])\n",
    "    y_train.append(1)\n",
    "    tokenized = tokenizer(text,topic_class_hypothesis[int(label_rand)],return_tensors=\"tf\",truncation=True, padding=\"max_length\")\n",
    "    bert_feature = bert(tokenized,output_hidden_states=True)\n",
    "    x_train.append(bert_feature.pooler_output[0])\n",
    "    y_train.append(0)\n",
    "\n",
    "np.save(X_TRAIN, x_train)\n",
    "np.save(Y_TRAIN, y_train)\n",
    "x_train, y_train=0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia class ------------------------------------------------------------------------------------------------------\n",
    "with open('../data/dbpedia_csv/classes.txt','r',encoding='utf-8') as f:\n",
    "    dbpedia_class = ['this text is about '+text for text in f.read().splitlines()]\n",
    "\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "import random\n",
    "reader = random.sample(reader,1000)\n",
    "# #----------------------------\n",
    "\n",
    "x_test, y_test = [],[]\n",
    "first, second = [],[]\n",
    "for cls_num,auth,readtext in tqdm(reader,total=len(reader)):\n",
    "    for db_class in dbpedia_class:\n",
    "        first.append(preprocessing(readtext,auth))\n",
    "        second.append(db_class)\n",
    "        tokenized = tokenizer(preprocessing(readtext,auth), db_class, return_tensors=\"tf\", truncation=True, padding=\"max_length\")\n",
    "        bert_feature = bert(tokenized, output_hidden_states=True)\n",
    "        x_test.append(bert_feature.pooler_output[0])\n",
    "        \n",
    "    y_test.append(int(cls_num))\n",
    "\n",
    "np.save(X_TEST, x_test)\n",
    "np.save(Y_TEST, y_test)\n",
    "x_test, y_test = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(X_TRAIN)\n",
    "y_train = np.load(Y_TRAIN)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(768,))\n",
    "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(inputs)\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss='binary_crossentropy',metrics=['mae','mse','acc'])\n",
    "model.fit(x_train, y_train, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "model.save(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(X_TEST)\n",
    "y_test = np.load(Y_TEST)\n",
    "\n",
    "model = tf.keras.models.load_model(SAVED_MODEL)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "split_pred = np.array_split(pred,len(y_test))\n",
    "y_pred = [np.argmax(p)+1 for p in split_pred]\n",
    "\n",
    "labels = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "target_class = [\"Com.\",\"Edu.\",\"Art.\",\"Ath.\",\"Off.\",\"Mea.\",\"Bui.\",\"Nat.\",\"Vil.\",\"Ani.\",\"Pla.\",\"Alb.\",\"Fil.\",\"Wri.\"]\n",
    "rep = metrics.classification_report(y_test,y_pred,labels=labels,target_names=target_class,digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e32349538976b425bbf8209bec3a52ef38eb988b8b568d4d0fb100f86dbbec2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
