{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbl/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "X_TRAIN = '../dataset/HF-BERT_x_train.npy'\n",
    "Y_TRAIN = '../dataset/HF-BERT_y_train.npy'\n",
    "X_TEST = '../dataset/HF-BERT_x_test.npy'\n",
    "Y_TEST = '../dataset/HF-BERT_y_test.npy'\n",
    "SAVED_MODEL = \"../Baseline-HF-BERT.h5\"\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300000/1300000 [01:01<00:00, 21165.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing train data -----------------------------------------------------------------------\n",
    "# load topic class labels\n",
    "print(\"making train dataset...\")\n",
    "with open('../data/topic/classes.txt','r',encoding='utf-8') as f:\n",
    "    labels = f.read().splitlines()\n",
    "topic_class_hypothesis = dict()\n",
    "for i,label in enumerate(labels):\n",
    "    topic_class_hypothesis[i] = 'this text is about ' + ' or '.join([wordnet.synsets(word)[0].definition() for word in label.split(' & ')])\n",
    "\n",
    "# load train data\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "\n",
    "# # ## example -------------------------------------\n",
    "# import random\n",
    "# texts = texts.splitlines()\n",
    "# texts = random.sample(texts,10000)\n",
    "# texts = \"\\n\".join(texts)\n",
    "# # ## ---------------------------------------------\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "x_train, y_train = [],[]\n",
    "first, second = [],[]\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "    label,text = label_text.split('\\t')\n",
    "    rand_base = [0,1,2,3,4,5,6,7,8,9]\n",
    "    rand_base.remove(int(label))\n",
    "    label_rand = np.random.choice(rand_base)\n",
    "    first.append(preprocessing(text))\n",
    "    second.append(topic_class_hypothesis[int(label)])\n",
    "    y_train.append(1)\n",
    "    first.append(preprocessing(text))\n",
    "    second.append(topic_class_hypothesis[int(label_rand)])\n",
    "    y_train.append(0)\n",
    "\n",
    "x_train = tokenizer(first, second, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162500/162500 [==============================] - 70590s 434ms/step - loss: 0.2513 - binary_accuracy: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff5cfec518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.classifier = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"classifier\")\n",
    "model.compile(optimizer=keras.optimizers.Adam(3e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "model.fit([x_train[\"input_ids\"],x_train[\"attention_mask\"]], y_train, epochs=EPOCH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48070368]\n",
      " [0.03339637]\n",
      " [0.9951605 ]\n",
      " ...\n",
      " [0.03262991]\n",
      " [0.9243892 ]\n",
      " [0.5926784 ]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.907     0.919     0.913   1300000\n",
      "           1      0.918     0.906     0.912   1300000\n",
      "\n",
      "    accuracy                          0.912   2600000\n",
      "   macro avg      0.912     0.912     0.912   2600000\n",
      "weighted avg      0.912     0.912     0.912   2600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([x_train[\"input_ids\"],x_train[\"attention_mask\"]], batch_size=BATCH_SIZE)\n",
    "print(pred.logits)\n",
    "y_pred = np.where(pred.logits<0.5, 0, 1)\n",
    "\n",
    "rep = metrics.classification_report(y_train,y_pred,digits=3)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:13<00:00, 5342.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia class ------------------------------------------------------------------------------------------------------\n",
    "with open('../data/dbpedia_csv/classes.txt','r',encoding='utf-8') as f:\n",
    "    classes = f.read().splitlines()\n",
    "    dbpedia_class = ['this text is about '+text for text in classes]\n",
    "\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader,1000)\n",
    "# # #----------------------------\n",
    "\n",
    "x_test, y_test = [],[]\n",
    "first, second = [],[]\n",
    "for cls_num,auth,readtext in tqdm(reader,total=len(reader)):\n",
    "    for db_class in dbpedia_class:\n",
    "        text = readtext.replace(auth, \"\")\n",
    "        first.append(preprocessing(text))\n",
    "        second.append(db_class)\n",
    "    y_test.append(int(cls_num)-1)           \n",
    "\n",
    "x_test = tokenizer(first, second, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.436     0.310     0.362      5000\n",
      "        Edu.      0.187     0.606     0.285      5000\n",
      "        Art.      0.232     0.095     0.135      5000\n",
      "        Ath.      0.692     0.780     0.734      5000\n",
      "        Off.      0.091     0.013     0.022      5000\n",
      "        Mea.      0.379     0.069     0.117      5000\n",
      "        Bui.      0.165     0.187     0.176      5000\n",
      "        Nat.      0.133     0.394     0.199      5000\n",
      "        Vil.      0.028     0.002     0.003      5000\n",
      "        Ani.      0.361     0.479     0.412      5000\n",
      "        Pla.      0.174     0.106     0.132      5000\n",
      "        Alb.      0.772     0.235     0.361      5000\n",
      "        Fil.      0.383     0.529     0.444      5000\n",
      "        Wri.      0.138     0.055     0.078      5000\n",
      "\n",
      "    accuracy                          0.276     70000\n",
      "   macro avg      0.298     0.276     0.247     70000\n",
      "weighted avg      0.298     0.276     0.247     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([x_test[\"input_ids\"],x_test[\"attention_mask\"]], batch_size=BATCH_SIZE)\n",
    "split_pred = np.array_split(pred.logits,len(y_test))\n",
    "y_pred = [np.argmax(p) for p in split_pred]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "rep = metrics.classification_report(y_test,y_pred,target_names=target_names,digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('3.6.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11d1de90a7344cdfbee299251a47ba7fb912949f3086a91bbe84e05957082a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
